{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Chen-X666/novelWordDiscovery_BERT-wwm_emotionAnalysis/blob/master/newWord%2Bbert_wmm%2BemotionAnalysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVJrHhdc-IE6"
      },
      "source": [
        "# 新词发现"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vctu81954Xgc",
        "outputId": "e21cc5b0-b5d7-4736-cf27-7cf15270a306"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "tmNKEnKSAU8T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed9976db-8031-45fb-dc5c-dac785c29afc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/DLModel/DanmakuMarked-data-main.zip\n",
            "   creating: DanmakuMarked-data-main/\n",
            "   creating: DanmakuMarked-data-main/marked/\n",
            "  inflating: DanmakuMarked-data-main/marked/BV16f4y187Xj.csv  \n",
            "  inflating: DanmakuMarked-data-main/marked/BV16g411G7s6.csv  \n",
            "  inflating: DanmakuMarked-data-main/marked/BV19X4y1c7uM.csv  \n",
            "  inflating: DanmakuMarked-data-main/marked/BV1AM4y1M71p.csv  \n",
            "  inflating: DanmakuMarked-data-main/marked/BV1bV411x7Tm.csv  \n",
            "  inflating: DanmakuMarked-data-main/marked/BV1dw411o7X7.csv  \n",
            "  inflating: DanmakuMarked-data-main/marked/BV1i64y167av.csv  \n",
            "  inflating: DanmakuMarked-data-main/marked/BV1iv411H7Lt.csv  \n",
            "  inflating: DanmakuMarked-data-main/marked/BV1Ks41197Ne.csv  \n",
            "  inflating: DanmakuMarked-data-main/marked/BV1ms411b7Ph(0).csv  \n",
            "  inflating: DanmakuMarked-data-main/marked/BV1ms411b7Ph(1).csv  \n",
            "  inflating: DanmakuMarked-data-main/marked/BV1ms411b7Ph(2).csv  \n",
            "  inflating: DanmakuMarked-data-main/marked/BV1ms411b7Ph(3).csv  \n",
            "  inflating: DanmakuMarked-data-main/marked/BV1ms411b7Ph(4).csv  \n",
            "  inflating: DanmakuMarked-data-main/marked/BV1ms411b7Ph(5).csv  \n",
            "  inflating: DanmakuMarked-data-main/marked/BV1ms411b7Ph(6).csv  \n",
            "  inflating: DanmakuMarked-data-main/marked/BV1ms411b7Ph(7).csv  \n",
            "  inflating: DanmakuMarked-data-main/marked/BV1ms411b7Ph(8).csv  \n",
            "  inflating: DanmakuMarked-data-main/marked/BV1ms411b7Ph(9).csv  \n",
            "  inflating: DanmakuMarked-data-main/marked/BV1nV411W7Zh.csv  \n",
            "  inflating: DanmakuMarked-data-main/marked/BV1qp4y1t7DJ.csv  \n",
            "  inflating: DanmakuMarked-data-main/marked/BV1So4y1y7ch.csv  \n",
            "  inflating: DanmakuMarked-data-main/marked/BV1xW411U7JQ(0).csv  \n",
            "  inflating: DanmakuMarked-data-main/marked/BV1xW411U7JQ(1).csv  \n",
            "  inflating: DanmakuMarked-data-main/marked/BV1xW411U7JQ(2).csv  \n",
            "  inflating: DanmakuMarked-data-main/marked/BV1xW411U7JQ(3).csv  \n",
            "  inflating: DanmakuMarked-data-main/marked/BV1xW411U7JQ(4).csv  \n",
            "  inflating: DanmakuMarked-data-main/marked/BV1xW411U7JQ(5).csv  \n",
            "  inflating: DanmakuMarked-data-main/marked/BV1xW411U7JQ(6).csv  \n",
            "  inflating: DanmakuMarked-data-main/marked/BV1xW411U7JQ(7).csv  \n",
            "  inflating: DanmakuMarked-data-main/marked/BV1xW411U7JQ(8).csv  \n",
            "  inflating: DanmakuMarked-data-main/marked/BV1XZ4y1F7eG.csv  \n",
            "  inflating: DanmakuMarked-data-main/marked/BV1y64y167Sf.csv  \n",
            "  inflating: DanmakuMarked-data-main/marked/BV1Yv411H7AV.csv  \n",
            "  inflating: DanmakuMarked-data-main/README.md  \n",
            "  inflating: DanmakuMarked-data-main/sample.csv  \n",
            "   creating: DanmakuMarked-data-main/unmarked/\n",
            "  inflating: DanmakuMarked-data-main/unmarked/307364844.csv  \n",
            "  inflating: DanmakuMarked-data-main/unmarked/307366843.csv  \n",
            "  inflating: DanmakuMarked-data-main/unmarked/307367343.csv  \n",
            "  inflating: DanmakuMarked-data-main/unmarked/307367843.csv  \n",
            "  inflating: DanmakuMarked-data-main/unmarked/307368343.csv  \n",
            "  inflating: DanmakuMarked-data-main/unmarked/307368843.csv  \n",
            "  inflating: DanmakuMarked-data-main/unmarked/307369343.csv  \n",
            "  inflating: DanmakuMarked-data-main/unmarked/307369843.csv  \n",
            "  inflating: DanmakuMarked-data-main/unmarked/307370343.csv  \n",
            "  inflating: DanmakuMarked-data-main/unmarked/307370843.csv  \n",
            "  inflating: DanmakuMarked-data-main/unmarked/307371343.csv  \n",
            "  inflating: DanmakuMarked-data-main/unmarked/307371843.csv  \n",
            "  inflating: DanmakuMarked-data-main/unmarked/307372343.csv  \n",
            "  inflating: DanmakuMarked-data-main/unmarked/307372843.csv  \n",
            "  inflating: DanmakuMarked-data-main/unmarked/307373343.csv  \n",
            "  inflating: DanmakuMarked-data-main/unmarked/307373843.csv  \n",
            "  inflating: DanmakuMarked-data-main/unmarked/307374343.csv  \n",
            "  inflating: DanmakuMarked-data-main/unmarked/307374843.csv  \n",
            "  inflating: DanmakuMarked-data-main/unmarked/307375343.csv  \n",
            "  inflating: DanmakuMarked-data-main/unmarked/307375843.csv  \n",
            "  inflating: DanmakuMarked-data-main/unmarked/307376343.csv  \n",
            "  inflating: DanmakuMarked-data-main/unmarked/307376843.csv  \n",
            "  inflating: DanmakuMarked-data-main/unmarked/307377343.csv  \n",
            "  inflating: DanmakuMarked-data-main/unmarked/307377843.csv  \n",
            "  inflating: DanmakuMarked-data-main/unmarked/307378343.csv  \n",
            "  inflating: DanmakuMarked-data-main/unmarked/307378843.csv  \n",
            "  inflating: DanmakuMarked-data-main/unmarked/307379343.csv  \n",
            "  inflating: DanmakuMarked-data-main/unmarked/307390843.csv  \n",
            "  inflating: DanmakuMarked-data-main/unmarked/307391343.csv  \n",
            "  inflating: DanmakuMarked-data-main/unmarked/307391843.csv  \n",
            "  inflating: DanmakuMarked-data-main/unmarked/307392343.csv  \n",
            "  inflating: DanmakuMarked-data-main/unmarked/307392843.csv  \n",
            "  inflating: DanmakuMarked-data-main/unmarked/307393343.csv  \n",
            "  inflating: DanmakuMarked-data-main/unmarked/307393843.csv  \n",
            "  inflating: DanmakuMarked-data-main/unmarked/307394343.csv  \n",
            "  inflating: DanmakuMarked-data-main/unmarked/307394843.csv  \n",
            "  inflating: DanmakuMarked-data-main/unmarked/307395343.csv  \n",
            "  inflating: DanmakuMarked-data-main/unmarked/307395843.csv  \n",
            "  inflating: DanmakuMarked-data-main/unmarked/307396343.csv  \n",
            "  inflating: DanmakuMarked-data-main/unmarked/307396843.csv  \n",
            "  inflating: DanmakuMarked-data-main/unmarked/307397343.csv  \n",
            "  inflating: DanmakuMarked-data-main/unmarked/307397843.csv  \n",
            "  inflating: DanmakuMarked-data-main/unmarked/307410343.csv  \n",
            "  inflating: DanmakuMarked-data-main/unmarked/307410843.csv  \n",
            "  inflating: DanmakuMarked-data-main/unmarked/307411343.csv  \n",
            "  inflating: DanmakuMarked-data-main/unmarked/307411843.csv  \n",
            "  inflating: DanmakuMarked-data-main/unmarked/307412343.csv  \n",
            "  inflating: DanmakuMarked-data-main/unmarked/307412843.csv  \n",
            "  inflating: DanmakuMarked-data-main/unmarked/307413343.csv  \n",
            "  inflating: DanmakuMarked-data-main/unmarked/307413843.csv  \n",
            "  inflating: DanmakuMarked-data-main/unmarked/307414343.csv  \n",
            "  inflating: DanmakuMarked-data-main/unmarked/307414843.csv  \n",
            "  inflating: DanmakuMarked-data-main/unmarked/307415343.csv  \n",
            "  inflating: DanmakuMarked-data-main/unmarked/307415843.csv  \n",
            "  inflating: DanmakuMarked-data-main/unmarked/307416343.csv  \n",
            "  inflating: DanmakuMarked-data-main/unmarked/307416843.csv  \n",
            "  inflating: DanmakuMarked-data-main/unmarked/307417343.csv  \n",
            "  inflating: DanmakuMarked-data-main/unmarked/307417843.csv  \n",
            "  inflating: DanmakuMarked-data-main/unmarked/307418343.csv  \n",
            "  inflating: DanmakuMarked-data-main/unmarked/307418843.csv  \n",
            "  inflating: DanmakuMarked-data-main/unmarked/307419343.csv  \n",
            "  inflating: DanmakuMarked-data-main/unmarked/307419843.csv  \n",
            "  inflating: DanmakuMarked-data-main/unmarked/307420343.csv  \n",
            "  inflating: DanmakuMarked-data-main/unmarked/307420843.csv  \n",
            "  inflating: DanmakuMarked-data-main/unmarked/307421343.csv  \n",
            "  inflating: DanmakuMarked-data-main/unmarked/307421843.csv  \n",
            "  inflating: DanmakuMarked-data-main/unmarked/307422343.csv  \n",
            "  inflating: DanmakuMarked-data-main/unmarked/307422843.csv  \n",
            "  inflating: DanmakuMarked-data-main/unmarked/307423343.csv  \n",
            "  inflating: DanmakuMarked-data-main/unmarked/307423843.csv  \n",
            "  inflating: DanmakuMarked-data-main/unmarked/307424343.csv  \n",
            "  inflating: DanmakuMarked-data-main/unmarked/307424843.csv  \n",
            "  inflating: DanmakuMarked-data-main/unmarked/307425343.csv  \n",
            "  inflating: DanmakuMarked-data-main/unmarked/307425843.csv  \n",
            "  inflating: DanmakuMarked-data-main/unmarked/307426343.csv  \n",
            "  inflating: DanmakuMarked-data-main/unmarked/307426843.csv  \n",
            "  inflating: DanmakuMarked-data-main/unmarked/307427343.csv  \n",
            "  inflating: DanmakuMarked-data-main/unmarked/307427843.csv  \n",
            "  inflating: DanmakuMarked-data-main/unmarked/307428343.csv  \n",
            "  inflating: DanmakuMarked-data-main/unmarked/307428843.csv  \n",
            "  inflating: DanmakuMarked-data-main/unmarked/307429343.csv  \n"
          ]
        }
      ],
      "source": [
        "! unzip /content/drive/MyDrive/DLModel/DanmakuMarked-data-main.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Gf4CFGNH-K8V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b7b2d1e-2e29-41ed-97d8-ba57fa2a8447"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'novelWordDiscovery_BERT-wwm_emotionAnalysis'...\n",
            "remote: Enumerating objects: 50, done.\u001b[K\n",
            "remote: Counting objects: 100% (50/50), done.\u001b[K\n",
            "remote: Compressing objects: 100% (30/30), done.\u001b[K\n",
            "remote: Total 50 (delta 18), reused 50 (delta 18), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (50/50), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Chen-X666/novelWordDiscovery_BERT-wwm_emotionAnalysis.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install flashtext"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CdBxzS7ZxB5l",
        "outputId": "c6ed7cbd-020b-43f9-f4be-39bcd6dee730"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting flashtext\n",
            "  Downloading flashtext-2.7.tar.gz (14 kB)\n",
            "Building wheels for collected packages: flashtext\n",
            "  Building wheel for flashtext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for flashtext: filename=flashtext-2.7-py2.py3-none-any.whl size=9309 sha256=9fceca90c1fd79c0ca06af75a556b1a4fe324d88de5388b59464db211e97fc72\n",
            "  Stored in directory: /root/.cache/pip/wheels/cb/19/58/4e8fdd0009a7f89dbce3c18fff2e0d0fa201d5cdfd16f113b7\n",
            "Successfully built flashtext\n",
            "Installing collected packages: flashtext\n",
            "Successfully installed flashtext-2.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/novelWordDiscovery_BERT-wwm_emotionAnalysis/NovelWordDiscovery/demo.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mz6A-Kg5w7pn",
        "outputId": "dea71662-9cc6-4d3b-d4e8-733391bbe15d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-05-12 09:53:26,430 INFO    : NewWordDiscover Setting Arguments Values:  \n",
            "     CWD: /content/novelWordDiscovery_BERT-wwm_emotionAnalysis/NovelWordDiscovery\n",
            "     Call_Time: 20220512095326\n",
            "     batch_len: 10000000\n",
            "     co_min: 5\n",
            "     emojiCorpus: emojis.csv\n",
            "     f_data_col: Content\n",
            "     f_encoding: utf-8\n",
            "     f_txt_sep: \n",
            "\n",
            "     file_name: BV19X4y1c7uM.csv\n",
            "     h_min: 1.2\n",
            "     level_f: DEBUG\n",
            "     level_s: INFO\n",
            "     n_gram: 5\n",
            "     p_min: 8\n",
            "     path_corpus: BV19X4y1c7uM.csv\n",
            "     path_log: NLP_NewWordDiscover_20220512095326.Log\n",
            "     start_time: 1652349206.4064813\n",
            "     top_n: 10000000\n",
            "     videoLength: 3\n",
            "2022-05-12 09:53:26,430 INFO    : NewWordDiscover Starting......\n",
            "2022-05-12 09:53:26,430 INFO    : - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "2022-05-12 09:53:26,430 INFO    :  2 进程 n_gram 词组统计程序开始。。。。\n",
            "2022-05-12 09:53:26,468 INFO    : 进程 1/6 进入处理池。。。\n",
            "2022-05-12 09:53:26,471 INFO    : 进程 2/6 进入处理池。。。\n",
            "2022-05-12 09:53:26,473 INFO    : DataPretreatment——————————loading file——————————DataPretreatment\n",
            "2022-05-12 09:53:26,476 INFO    : DataPretreatment——————————loading file——————————DataPretreatment\n",
            "2022-05-12 09:53:26,500 INFO    : DataPretreatment——————————building emojis adTrie——————————DataPretreatment\n",
            "2022-05-12 09:53:26,503 INFO    : DataPretreatment——————————building emojis adTrie——————————DataPretreatment\n",
            "2022-05-12 09:53:26,870 INFO    : NewWordResult path:  /content/novelWordDiscovery_BERT-wwm_emotionAnalysis/NovelWordDiscovery/NewWordResult/NewWordResult_BV19X4y1c7uM.csv.csv  \n",
            "2022-05-12 09:53:26,871 INFO    : DataPretreatment——————————emojis discovery finished——————————DataPretreatment\n",
            "2022-05-12 09:53:26,871 INFO    : DataPretreatment——————————Starting Clean(tra2sim+cap2low)——————————DataPretreatment\n",
            "2022-05-12 09:53:26,885 INFO    : NewWordResult path:  /content/novelWordDiscovery_BERT-wwm_emotionAnalysis/NovelWordDiscovery/NewWordResult/NewWordResult_BV19X4y1c7uM.csv.csv  \n",
            "2022-05-12 09:53:26,885 INFO    : DataPretreatment——————————emojis discovery finished——————————DataPretreatment\n",
            "2022-05-12 09:53:26,885 INFO    : DataPretreatment——————————Starting Clean(tra2sim+cap2low)——————————DataPretreatment\n",
            "  1 WordWindows: 1  line: 3597      2022-05-12 09:53:27,273 INFO    : Process_i  1  Finish!    \n",
            "2022-05-12 09:53:27,278 INFO    : Process_i  2  Finish!    \n",
            "2022-05-12 09:53:27,475 INFO    : 进程 3/6 进入处理池。。。\n",
            "2022-05-12 09:53:27,477 INFO    : 进程 4/6 进入处理池。。。\n",
            "2022-05-12 09:53:27,480 INFO    : DataPretreatment——————————loading file——————————DataPretreatment\n",
            "2022-05-12 09:53:27,485 INFO    : DataPretreatment——————————loading file——————————DataPretreatment\n",
            "2022-05-12 09:53:27,509 INFO    : DataPretreatment——————————building emojis adTrie——————————DataPretreatment\n",
            "2022-05-12 09:53:27,513 INFO    : DataPretreatment——————————building emojis adTrie——————————DataPretreatment\n",
            "2022-05-12 09:53:27,884 INFO    : NewWordResult path:  /content/novelWordDiscovery_BERT-wwm_emotionAnalysis/NovelWordDiscovery/NewWordResult/NewWordResult_BV19X4y1c7uM.csv.csv  \n",
            "2022-05-12 09:53:27,884 INFO    : DataPretreatment——————————emojis discovery finished——————————DataPretreatment\n",
            "2022-05-12 09:53:27,884 INFO    : DataPretreatment——————————Starting Clean(tra2sim+cap2low)——————————DataPretreatment\n",
            "2022-05-12 09:53:27,891 INFO    : NewWordResult path:  /content/novelWordDiscovery_BERT-wwm_emotionAnalysis/NovelWordDiscovery/NewWordResult/NewWordResult_BV19X4y1c7uM.csv.csv  \n",
            "2022-05-12 09:53:27,891 INFO    : DataPretreatment——————————emojis discovery finished——————————DataPretreatment\n",
            "2022-05-12 09:53:27,891 INFO    : DataPretreatment——————————Starting Clean(tra2sim+cap2low)——————————DataPretreatment\n",
            "  3 WordWindows: 3  line: 3597      2022-05-12 09:53:28,299 INFO    : Process_i  4  Finish!    \n",
            "2022-05-12 09:53:28,302 INFO    : Process_i  3  Finish!    \n",
            "2022-05-12 09:53:28,481 INFO    : 进程 5/6 进入处理池。。。\n",
            "2022-05-12 09:53:28,484 INFO    : 进程 6/6 进入处理池。。。\n",
            "2022-05-12 09:53:28,486 INFO    : DataPretreatment——————————loading file——————————DataPretreatment\n",
            "2022-05-12 09:53:28,490 INFO    : DataPretreatment——————————loading file——————————DataPretreatment\n",
            "2022-05-12 09:53:28,516 INFO    : DataPretreatment——————————building emojis adTrie——————————DataPretreatment\n",
            "2022-05-12 09:53:28,519 INFO    : DataPretreatment——————————building emojis adTrie——————————DataPretreatment\n",
            "2022-05-12 09:53:28,896 INFO    : NewWordResult path:  /content/novelWordDiscovery_BERT-wwm_emotionAnalysis/NovelWordDiscovery/NewWordResult/NewWordResult_BV19X4y1c7uM.csv.csv  \n",
            "2022-05-12 09:53:28,896 INFO    : DataPretreatment——————————emojis discovery finished——————————DataPretreatment\n",
            "2022-05-12 09:53:28,897 INFO    : DataPretreatment——————————Starting Clean(tra2sim+cap2low)——————————DataPretreatment\n",
            "2022-05-12 09:53:28,898 INFO    : NewWordResult path:  /content/novelWordDiscovery_BERT-wwm_emotionAnalysis/NovelWordDiscovery/NewWordResult/NewWordResult_BV19X4y1c7uM.csv.csv  \n",
            "2022-05-12 09:53:28,898 INFO    : DataPretreatment——————————emojis discovery finished——————————DataPretreatment\n",
            "2022-05-12 09:53:28,898 INFO    : DataPretreatment——————————Starting Clean(tra2sim+cap2low)——————————DataPretreatment\n",
            "  6 WordWindows: 6  line: 3597      2022-05-12 09:53:29,314 INFO    : Process_i  5  Finish!    \n",
            "2022-05-12 09:53:29,320 INFO    : Process_i  6  Finish!    \n",
            "2022-05-12 09:53:30,488 INFO    : 进程信息： {1: 'OVER', 2: 'OVER', 4: 'OVER', 3: 'OVER', 5: 'OVER', 6: 'OVER'} \n",
            "2022-05-12 09:53:30,488 INFO    : - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "2022-05-12 09:53:30,489 INFO    : 2 n_gram  词出现最小概率 p_min:8  最小凝聚度 co_min:5  最小自由度 h_min:1.2  \n",
            "2022-05-12 09:53:30,489 INFO    : 3 n_gram  词出现最小概率 p_min:8  最小凝聚度 co_min:5  最小自由度 h_min:1.2  \n",
            "2022-05-12 09:53:30,490 INFO    : 4 n_gram  词出现最小概率 p_min:8  最小凝聚度 co_min:5  最小自由度 h_min:1.2  \n",
            "2022-05-12 09:53:30,490 INFO    : 5 n_gram  词出现最小概率 p_min:8  最小凝聚度 co_min:5  最小自由度 h_min:1.2  \n",
            "2022-05-12 09:53:30,490 INFO    : - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "2022-05-12 09:53:30,490 INFO    :  2 进程 n_gram 新词发现程序开始。。。。\n",
            "2022-05-12 09:53:30,491 INFO    : 进程 1/4 进入处理池。。。\n",
            "2022-05-12 09:53:30,493 INFO    : 进程 2/4 进入处理池。。。\n",
            "2022-05-12 09:53:30,520 INFO    : 文本字符串总长度： 27886 \n",
            "2022-05-12 09:53:30,520 INFO    : 2 n_gram  满足最低概率词数量： 415 \n",
            " 0: 1         2022-05-12 09:53:30,533 INFO    : 3 n_gram  满足最低概率词数量： 114 \n",
            " 1: 113       2022-05-12 09:53:30,864 INFO    : Process_i 1  Finish!    \n",
            " 0: 207       2022-05-12 09:53:31,498 INFO    : 进程 3/4 进入处理池。。。\n",
            " 0: 219       2022-05-12 09:53:31,542 INFO    : 4 n_gram  满足最低概率词数量： 39 \n",
            " 2: 38        2022-05-12 09:53:31,720 INFO    : Process_i 2  Finish!    \n",
            " 0: 414       2022-05-12 09:53:32,113 INFO    : Process_i 0  Finish!    \n",
            "2022-05-12 09:53:32,503 INFO    : 进程 4/4 进入处理池。。。\n",
            "2022-05-12 09:53:32,505 INFO    : 进程 4 等待结束中。。。   \n",
            "2022-05-12 09:53:32,525 INFO    : 5 n_gram  满足最低概率词数量： 20 \n",
            " 3: 19        2022-05-12 09:53:32,543 INFO    : Process_i 3  Finish!    \n",
            "2022-05-12 09:53:34,549 INFO    : 进程信息： {1: 'OVER', 2: 'OVER', 0: 'OVER', 3: 'OVER'} \n",
            "2022-05-12 09:53:34,549 INFO    : - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "2022-05-12 09:53:34,549 INFO    : CandidateWordResult File:  CandidateWordResult_BV19X4y1c7uM.csv_2_ngram.tmp  WordNum: 68\n",
            "2022-05-12 09:53:34,549 INFO    : CandidateWordResult File:  CandidateWordResult_BV19X4y1c7uM.csv_3_ngram.tmp  WordNum: 3\n",
            "2022-05-12 09:53:34,550 INFO    : CandidateWordResult File:  CandidateWordResult_BV19X4y1c7uM.csv_4_ngram.tmp  WordNum: 2\n",
            "2022-05-12 09:53:34,550 INFO    : CandidateWordResult File:  CandidateWordResult_BV19X4y1c7uM.csv_5_ngram.tmp  WordNum: 0\n",
            "2022-05-12 09:53:34,551 INFO    : CandidateWordResult path:  /content/novelWordDiscovery_BERT-wwm_emotionAnalysis/NovelWordDiscovery/CandidateWordResult/CandidateWordResult_BV19X4y1c7uM.csv.csv  \n",
            "2022-05-12 09:53:34,555 INFO    : NewWordResult path:  /content/novelWordDiscovery_BERT-wwm_emotionAnalysis/NovelWordDiscovery/NewWordResult/NewWordResult_BV19X4y1c7uM.csv.csv  \n",
            "2022-05-12 09:53:34,555 INFO    : NewWordDiscover Finish！ UsedTime 8.1 Sec.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "text = pd.read_csv('/content/novelWordDiscovery_BERT-wwm_emotionAnalysis/NovelWordDiscovery/Data/BV19X4y1c7uM.csv',encoding = 'utf-8')['Content'].to_csv('text.csv',encoding='utf-8',index=False)"
      ],
      "metadata": {
        "id": "km4epj45tnEF"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOsF7rx583em"
      },
      "source": [
        "# bert-wmm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vD-3oeEpH0yA",
        "outputId": "a9964eed-2bca-4aac-95c2-884791ab6549"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'NLP-Series-NewWordsMining-PTMPretraining'...\n",
            "remote: Enumerating objects: 12, done.\u001b[K\n",
            "remote: Counting objects: 100% (12/12), done.\u001b[K\n",
            "remote: Compressing objects: 100% (10/10), done.\u001b[K\n",
            "remote: Total 12 (delta 0), reused 12 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (12/12), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/zhoujx4/NLP-Series-NewWordsMining-PTMPretraining.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LhAZ_V7b5j1V",
        "outputId": "10866119-c2a3-47c7-8840-91828524bb84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ltp\n",
            "  Downloading ltp-4.1.5.post2-py3-none-any.whl (94 kB)\n",
            "\u001b[?25l\r\u001b[K     |███▌                            | 10 kB 19.5 MB/s eta 0:00:01\r\u001b[K     |███████                         | 20 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 30 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 40 kB 3.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 51 kB 3.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 61 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 71 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 81 kB 4.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 92 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 94 kB 2.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from ltp) (21.3)\n",
            "Requirement already satisfied: torch>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from ltp) (1.11.0+cu113)\n",
            "Collecting transformers<=4.7.0,>=4.0.0\n",
            "  Downloading transformers-4.7.0-py3-none-any.whl (2.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.5 MB 24.6 MB/s \n",
            "\u001b[?25hCollecting pygtrie<2.5,>=2.3.0\n",
            "  Downloading pygtrie-2.4.2.tar.gz (35 kB)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->ltp) (3.0.8)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.2.0->ltp) (4.2.0)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[K     |████████████████████████████████| 880 kB 37.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers<=4.7.0,>=4.0.0->ltp) (4.64.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<=4.7.0,>=4.0.0->ltp) (3.6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<=4.7.0,>=4.0.0->ltp) (2019.12.20)\n",
            "Collecting huggingface-hub==0.0.8\n",
            "  Downloading huggingface_hub-0.0.8-py3-none-any.whl (34 kB)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from transformers<=4.7.0,>=4.0.0->ltp) (3.13)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers<=4.7.0,>=4.0.0->ltp) (1.21.6)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers<=4.7.0,>=4.0.0->ltp) (4.11.3)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 38.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers<=4.7.0,>=4.0.0->ltp) (2.23.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers<=4.7.0,>=4.0.0->ltp) (3.8.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<=4.7.0,>=4.0.0->ltp) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<=4.7.0,>=4.0.0->ltp) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<=4.7.0,>=4.0.0->ltp) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<=4.7.0,>=4.0.0->ltp) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<=4.7.0,>=4.0.0->ltp) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<=4.7.0,>=4.0.0->ltp) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<=4.7.0,>=4.0.0->ltp) (1.1.0)\n",
            "Building wheels for collected packages: pygtrie, sacremoses\n",
            "  Building wheel for pygtrie (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pygtrie: filename=pygtrie-2.4.2-py3-none-any.whl size=19063 sha256=16858fe888efbbc400720779fa0347125f83721ee94f8a11416fa149b4d0bd1f\n",
            "  Stored in directory: /root/.cache/pip/wheels/d3/f8/ba/1d828b1603ea422686eb694253a43cb3a5901ea4696c1e0603\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=e9780ca8a98082259d24af17dbef7c4641c6507390118a4d8be3ccb96ed6f715\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n",
            "Successfully built pygtrie sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, huggingface-hub, transformers, pygtrie, ltp\n",
            "Successfully installed huggingface-hub-0.0.8 ltp-4.1.5.post2 pygtrie-2.4.2 sacremoses-0.0.53 tokenizers-0.10.3 transformers-4.7.0\n"
          ]
        }
      ],
      "source": [
        "!pip install ltp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QftUmcfSF7bD",
        "outputId": "fcdcc96e-fab0-4db0-bd40-3ac50451f4f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/DLModel/chinese_wwm_L-12_H-768_A-12.zip\n",
            "   creating: publish/\n",
            "  inflating: publish/vocab.txt       \n",
            "  inflating: publish/bert_model.ckpt.index  \n",
            "  inflating: publish/bert_model.ckpt.data-00000-of-00001  \n",
            "  inflating: publish/bert_config.json  \n",
            "  inflating: publish/bert_model.ckpt.meta  \n",
            "Archive:  /content/drive/MyDrive/DLModel/train_dataset.zip\n",
            "  inflating: nCoV_100k_train.labled.csv  \n",
            "  inflating: nCoV_900k_train.unlabled.csv  \n"
          ]
        }
      ],
      "source": [
        "! unzip /content/drive/MyDrive/DLModel/chinese_wwm_L-12_H-768_A-12.zip\n",
        "! unzip /content/drive/MyDrive/DLModel/train_dataset.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U1s4ald85fWq",
        "outputId": "f3cdffd0-e5a3-4a0b-86dc-b86a978b881f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ltp_tokenizering: 2it [00:00,  2.12it/s]\n",
            "bert tokenizering: 2it [00:00, 22.21it/s]      \n"
          ]
        }
      ],
      "source": [
        "!python run_chinese_ref.py --file_name data/test.txt --bert /content/NLP-Series-NewWordsMining-PTMPretraining/publish --save_path ref.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hgbsHelpJG4h",
        "outputId": "cd2ffe83-96ad-4e61-8e35-a85486767695"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-2.2.1-py3-none-any.whl (342 kB)\n",
            "\u001b[K     |████████████████████████████████| 342 kB 4.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n",
            "Collecting huggingface-hub<1.0.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.6.0-py3-none-any.whl (84 kB)\n",
            "\u001b[K     |████████████████████████████████| 84 kB 3.2 MB/s \n",
            "\u001b[?25hCollecting xxhash\n",
            "  Downloading xxhash-3.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[K     |████████████████████████████████| 212 kB 51.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.11.3)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.21.6)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n",
            "Collecting fsspec[http]>=2021.05.0\n",
            "  Downloading fsspec-2022.3.0-py3-none-any.whl (136 kB)\n",
            "\u001b[K     |████████████████████████████████| 136 kB 52.8 MB/s \n",
            "\u001b[?25hCollecting aiohttp\n",
            "  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 30.4 MB/s \n",
            "\u001b[?25hCollecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.64.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.6.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.13)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 51.9 MB/s \n",
            "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n",
            "\u001b[K     |████████████████████████████████| 94 kB 391 kB/s \n",
            "\u001b[?25hCollecting asynctest==0.13.0\n",
            "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.0.12)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 53.4 MB/s \n",
            "\u001b[?25hCollecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.4.0)\n",
            "Collecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
            "\u001b[K     |████████████████████████████████| 144 kB 54.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.8.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2022.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Installing collected packages: multidict, frozenlist, yarl, urllib3, asynctest, async-timeout, aiosignal, fsspec, aiohttp, xxhash, responses, huggingface-hub, datasets\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.0.8\n",
            "    Uninstalling huggingface-hub-0.0.8:\n",
            "      Successfully uninstalled huggingface-hub-0.0.8\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "transformers 4.7.0 requires huggingface-hub==0.0.8, but you have huggingface-hub 0.6.0 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 asynctest-0.13.0 datasets-2.2.1 frozenlist-1.3.0 fsspec-2022.3.0 huggingface-hub-0.6.0 multidict-6.0.2 responses-0.18.0 urllib3-1.25.11 xxhash-3.0.0 yarl-1.7.2\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "6T0F_fbPtdfn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0b41739-bfce-4a26-d4d9-814ad4e1bc0e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/huggingface/transformers\n",
            "  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-lme2n5db\n",
            "  Running command git clone -q https://github.com/huggingface/transformers /tmp/pip-req-build-lme2n5db\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.19.0.dev0) (3.6.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.19.0.dev0) (4.64.0)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 4.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.19.0.dev0) (4.11.3)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 50.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.19.0.dev0) (21.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.19.0.dev0) (0.6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.19.0.dev0) (1.21.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.19.0.dev0) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.19.0.dev0) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.19.0.dev0) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers==4.19.0.dev0) (3.0.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.19.0.dev0) (3.8.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.19.0.dev0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.19.0.dev0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.19.0.dev0) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.19.0.dev0) (1.25.11)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building wheel for transformers (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.19.0.dev0-py3-none-any.whl size=4151529 sha256=7aae7973754e905908dcd3e566c39655fd24f8b08518f5f8045a5da5781a80e6\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-9sj5uk3w/wheels/35/2e/a7/d819e3310040329f0f47e57c9e3e7a7338aa5e74c49acfe522\n",
            "Successfully built transformers\n",
            "Installing collected packages: pyyaml, tokenizers, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.10.3\n",
            "    Uninstalling tokenizers-0.10.3:\n",
            "      Successfully uninstalled tokenizers-0.10.3\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.7.0\n",
            "    Uninstalling transformers-4.7.0:\n",
            "      Successfully uninstalled transformers-4.7.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ltp 4.1.5.post2 requires transformers<=4.7.0,>=4.0.0, but you have transformers 4.19.0.dev0 which is incompatible.\u001b[0m\n",
            "Successfully installed pyyaml-6.0 tokenizers-0.12.1 transformers-4.19.0.dev0\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/huggingface/transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V7QtHWhJC06y",
        "outputId": "38f1a5fa-3692-4ff7-a72a-006db06f2837"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: Failed to call git rev-parse --git-dir --show-toplevel: \"fatal: not a git repository (or any of the parent directories): .git\\n\"\n",
            "Git LFS initialized.\n",
            "Cloning into 'chinese-bert-wwm-ext'...\n",
            "remote: Enumerating objects: 54, done.\u001b[K\n",
            "remote: Counting objects: 100% (54/54), done.\u001b[K\n",
            "remote: Compressing objects: 100% (49/49), done.\u001b[K\n",
            "remote: Total 54 (delta 19), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (54/54), done.\n",
            "Filtering content: 100% (3/3), 1.14 GiB | 79.90 MiB/s, done.\n"
          ]
        }
      ],
      "source": [
        "!git lfs install\n",
        "!git clone https://huggingface.co/hfl/chinese-bert-wwm-ext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Spu8ACgF9I7a",
        "outputId": "7a7c302d-f4c5-47d2-ba04-82c07780389f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "05/12/2022 10:04:52 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "05/12/2022 10:04:52 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=10000,\n",
            "evaluation_strategy=IntervalStrategy.NO,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_min_num_params=0,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=16,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=HubStrategy.EVERY_SAVE,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/output_qalog/runs/May12_10-04-52_62aa261822f4,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=5.0,\n",
            "optim=OptimizerNames.ADAMW_HF,\n",
            "output_dir=/content/output_qalog,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=128,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/output_qalog,\n",
            "save_on_each_node=False,\n",
            "save_steps=5000,\n",
            "save_strategy=IntervalStrategy.STEPS,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "05/12/2022 10:04:53 - WARNING - datasets.builder - Using custom data configuration default-eb7b319100e28873\n",
            "05/12/2022 10:04:53 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n",
            "05/12/2022 10:04:53 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/text/default-eb7b319100e28873/0.0.0/4b86d314f7236db91f0a0f5cda32d4375445e64c5eda2692655dd99c2dac68e8\n",
            "05/12/2022 10:04:53 - WARNING - datasets.builder - Reusing dataset text (/root/.cache/huggingface/datasets/text/default-eb7b319100e28873/0.0.0/4b86d314f7236db91f0a0f5cda32d4375445e64c5eda2692655dd99c2dac68e8)\n",
            "05/12/2022 10:04:53 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/text/default-eb7b319100e28873/0.0.0/4b86d314f7236db91f0a0f5cda32d4375445e64c5eda2692655dd99c2dac68e8\n",
            "100% 2/2 [00:00<00:00, 929.07it/s]\n",
            "[INFO|configuration_utils.py:657] 2022-05-12 10:04:53,585 >> loading configuration file /content/chinese-bert-wwm-ext/config.json\n",
            "[INFO|configuration_utils.py:708] 2022-05-12 10:04:53,586 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"/content/chinese-bert-wwm-ext\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.19.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 21128\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:657] 2022-05-12 10:04:53,587 >> loading configuration file /content/chinese-bert-wwm-ext/config.json\n",
            "[INFO|configuration_utils.py:708] 2022-05-12 10:04:53,587 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"/content/chinese-bert-wwm-ext\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.19.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 21128\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1780] 2022-05-12 10:04:53,588 >> loading file /content/chinese-bert-wwm-ext/vocab.txt\n",
            "[INFO|tokenization_utils_base.py:1780] 2022-05-12 10:04:53,588 >> loading file /content/chinese-bert-wwm-ext/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:1780] 2022-05-12 10:04:53,588 >> loading file /content/chinese-bert-wwm-ext/added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:1780] 2022-05-12 10:04:53,588 >> loading file /content/chinese-bert-wwm-ext/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:1780] 2022-05-12 10:04:53,588 >> loading file /content/chinese-bert-wwm-ext/tokenizer_config.json\n",
            "[INFO|configuration_utils.py:657] 2022-05-12 10:04:53,588 >> loading configuration file /content/chinese-bert-wwm-ext/config.json\n",
            "[INFO|configuration_utils.py:708] 2022-05-12 10:04:53,589 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"/content/chinese-bert-wwm-ext\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.19.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 21128\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:1951] 2022-05-12 10:04:53,619 >> loading weights file /content/chinese-bert-wwm-ext/pytorch_model.bin\n",
            "[WARNING|modeling_utils.py:2255] 2022-05-12 10:04:55,550 >> Some weights of the model checkpoint at /content/chinese-bert-wwm-ext were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[INFO|modeling_utils.py:2272] 2022-05-12 10:04:55,550 >> All the weights of BertForMaskedLM were initialized from the model checkpoint at /content/chinese-bert-wwm-ext.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.\n",
            "Running tokenizer on every text in dataset #0:   0% 0/1 [00:00<?, ?ba/s]\n",
            "Running tokenizer on every text in dataset #0: 100% 1/1 [00:00<00:00, 10.60ba/s]\n",
            "\n",
            "\n",
            "Running tokenizer on every text in dataset #2:   0% 0/1 [00:00<?, ?ba/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Running tokenizer on every text in dataset #3:   0% 0/1 [00:00<?, ?ba/s]\u001b[A\u001b[A\u001b[A\n",
            "Running tokenizer on every text in dataset #1: 100% 1/1 [00:00<00:00,  8.91ba/s]\n",
            "Running tokenizer on every text in dataset #3: 100% 1/1 [00:00<00:00, 16.70ba/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on every text in dataset #4:   0% 0/1 [00:00<?, ?ba/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on every text in dataset #5:   0% 0/1 [00:00<?, ?ba/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Running tokenizer on every text in dataset #2: 100% 1/1 [00:00<00:00,  7.28ba/s]\n",
            "Running tokenizer on every text in dataset #5: 100% 1/1 [00:00<00:00, 19.25ba/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on every text in dataset #6:   0% 0/1 [00:00<?, ?ba/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on every text in dataset #7:   0% 0/1 [00:00<?, ?ba/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on every text in dataset #4: 100% 1/1 [00:00<00:00,  5.68ba/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on every text in dataset #6: 100% 1/1 [00:00<00:00, 11.08ba/s]\n",
            "Running tokenizer on every text in dataset #8: 100% 1/1 [00:00<00:00, 25.12ba/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on every text in dataset #9:   0% 0/1 [00:00<?, ?ba/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on every text in dataset #7: 100% 1/1 [00:00<00:00,  8.94ba/s]\n",
            "Running tokenizer on every text in dataset #9: 100% 1/1 [00:00<00:00, 29.11ba/s]\n",
            "\n",
            "Running tokenizer on every text in dataset #1: 100% 1/1 [00:00<00:00, 17.49ba/s]\n",
            "\n",
            "\n",
            "Running tokenizer on every text in dataset #0: 100% 1/1 [00:00<00:00, 10.24ba/s]\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on every text in dataset #2: 100% 1/1 [00:00<00:00, 12.39ba/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on every text in dataset #4:   0% 0/1 [00:00<?, ?ba/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on every text in dataset #5:   0% 0/1 [00:00<?, ?ba/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Running tokenizer on every text in dataset #3: 100% 1/1 [00:00<00:00,  7.99ba/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on every text in dataset #6:   0% 0/1 [00:00<?, ?ba/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on every text in dataset #7:   0% 0/1 [00:00<?, ?ba/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on every text in dataset #4: 100% 1/1 [00:00<00:00,  6.39ba/s]\n",
            "Running tokenizer on every text in dataset #5: 100% 1/1 [00:00<00:00, 10.39ba/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on every text in dataset #6: 100% 1/1 [00:00<00:00, 10.62ba/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on every text in dataset #7: 100% 1/1 [00:00<00:00, 10.91ba/s]\n",
            "Running tokenizer on every text in dataset #9: 100% 1/1 [00:00<00:00, 20.99ba/s]\n",
            "Running tokenizer on every text in dataset #8: 100% 1/1 [00:00<00:00, 13.54ba/s]\n",
            "Grouping texts in chunks of 64 #0:   0% 0/1 [00:00<?, ?ba/s]\n",
            "Grouping texts in chunks of 64 #1:   0% 0/1 [00:00<?, ?ba/s]\u001b[A\n",
            "\n",
            "Grouping texts in chunks of 64 #0: 100% 1/1 [00:00<00:00, 13.80ba/s]\n",
            "\n",
            "\n",
            "\n",
            "Grouping texts in chunks of 64 #1: 100% 1/1 [00:00<00:00, 14.46ba/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Grouping texts in chunks of 64 #3: 100% 1/1 [00:00<00:00, 16.87ba/s]\n",
            "\n",
            "\n",
            "Grouping texts in chunks of 64 #2: 100% 1/1 [00:00<00:00,  7.51ba/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Grouping texts in chunks of 64 #5:   0% 0/1 [00:00<?, ?ba/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Grouping texts in chunks of 64 #5: 100% 1/1 [00:00<00:00, 10.79ba/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Grouping texts in chunks of 64 #6: 100% 1/1 [00:00<00:00, 11.83ba/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Grouping texts in chunks of 64 #8:   0% 0/1 [00:00<?, ?ba/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Grouping texts in chunks of 64 #4: 100% 1/1 [00:00<00:00,  5.90ba/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Grouping texts in chunks of 64 #8: 100% 1/1 [00:00<00:00, 26.24ba/s]\n",
            "Grouping texts in chunks of 64 #7: 100% 1/1 [00:00<00:00, 11.74ba/s]\n",
            "Grouping texts in chunks of 64 #9: 100% 1/1 [00:00<00:00, 20.09ba/s]\n",
            "Grouping texts in chunks of 64 #0:   0% 0/1 [00:00<?, ?ba/s]\n",
            "Grouping texts in chunks of 64 #1:   0% 0/1 [00:00<?, ?ba/s]\u001b[A\n",
            "\n",
            "Grouping texts in chunks of 64 #0: 100% 1/1 [00:00<00:00, 14.38ba/s]\n",
            "\n",
            "\n",
            "\n",
            "Grouping texts in chunks of 64 #1: 100% 1/1 [00:00<00:00, 11.28ba/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Grouping texts in chunks of 64 #3: 100% 1/1 [00:00<00:00, 12.10ba/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Grouping texts in chunks of 64 #4: 100% 1/1 [00:00<00:00, 16.57ba/s]\n",
            "\n",
            "\n",
            "Grouping texts in chunks of 64 #2: 100% 1/1 [00:00<00:00,  6.28ba/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Grouping texts in chunks of 64 #6:   0% 0/1 [00:00<?, ?ba/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Grouping texts in chunks of 64 #5: 100% 1/1 [00:00<00:00, 13.15ba/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Grouping texts in chunks of 64 #8:   0% 0/1 [00:00<?, ?ba/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Grouping texts in chunks of 64 #9:   0% 0/1 [00:00<?, ?ba/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Grouping texts in chunks of 64 #8: 100% 1/1 [00:00<00:00, 15.20ba/s]\n",
            "Grouping texts in chunks of 64 #6: 100% 1/1 [00:00<00:00,  7.89ba/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Grouping texts in chunks of 64 #7: 100% 1/1 [00:00<00:00,  8.99ba/s]\n",
            "Grouping texts in chunks of 64 #9: 100% 1/1 [00:00<00:00, 14.74ba/s]\n",
            "Downloading builder script: 4.21kB [00:00, 4.24MB/s]       \n",
            "[INFO|trainer.py:623] 2022-05-12 10:05:13,531 >> The following columns in the training set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "[INFO|trainer.py:1419] 2022-05-12 10:05:13,550 >> ***** Running training *****\n",
            "[INFO|trainer.py:1420] 2022-05-12 10:05:13,550 >>   Num examples = 589\n",
            "[INFO|trainer.py:1421] 2022-05-12 10:05:13,550 >>   Num Epochs = 5\n",
            "[INFO|trainer.py:1422] 2022-05-12 10:05:13,550 >>   Instantaneous batch size per device = 128\n",
            "[INFO|trainer.py:1423] 2022-05-12 10:05:13,550 >>   Total train batch size (w. parallel, distributed & accumulation) = 2048\n",
            "[INFO|trainer.py:1424] 2022-05-12 10:05:13,550 >>   Gradient Accumulation steps = 16\n",
            "[INFO|trainer.py:1425] 2022-05-12 10:05:13,550 >>   Total optimization steps = 5\n",
            "100% 5/5 [00:18<00:00,  3.72s/it][INFO|trainer.py:1662] 2022-05-12 10:05:32,231 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 18.6815, 'train_samples_per_second': 157.642, 'train_steps_per_second': 0.268, 'train_loss': 0.9767491340637207, 'epoch': 5.0}\n",
            "100% 5/5 [00:18<00:00,  3.73s/it]\n",
            "[INFO|trainer.py:2340] 2022-05-12 10:05:32,234 >> Saving model checkpoint to /content/output_qalog\n",
            "[INFO|configuration_utils.py:446] 2022-05-12 10:05:32,235 >> Configuration saved in /content/output_qalog/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-12 10:05:33,642 >> Model weights saved in /content/output_qalog/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-05-12 10:05:33,642 >> tokenizer config file saved in /content/output_qalog/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-05-12 10:05:33,643 >> Special tokens file saved in /content/output_qalog/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        5.0\n",
            "  train_loss               =     0.9767\n",
            "  train_runtime            = 0:00:18.68\n",
            "  train_samples            =        589\n",
            "  train_samples_per_second =    157.642\n",
            "  train_steps_per_second   =      0.268\n",
            "05/12/2022 10:05:33 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:623] 2022-05-12 10:05:33,680 >> The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-05-12 10:05:33,685 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-05-12 10:05:33,685 >>   Num examples = 589\n",
            "[INFO|trainer.py:2595] 2022-05-12 10:05:33,685 >>   Batch size = 8\n",
            "100% 74/74 [00:01<00:00, 40.61it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        5.0\n",
            "  eval_accuracy           =     0.5289\n",
            "  eval_loss               =     2.4721\n",
            "  eval_runtime            = 0:00:01.84\n",
            "  eval_samples            =        589\n",
            "  eval_samples_per_second =    318.801\n",
            "  eval_steps_per_second   =     40.053\n",
            "  perplexity              =    11.8473\n",
            "[INFO|modelcard.py:460] 2022-05-12 10:05:36,286 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Masked Language Modeling', 'type': 'fill-mask'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.5289490932925497}]}\n"
          ]
        }
      ],
      "source": [
        "train_data=\"/content/text.txt\"\n",
        "valid_data=\"/content/text.txt\"\n",
        "pretrain_model=\"/content/chinese-bert-wwm-ext\"\n",
        "!python /content/drive/MyDrive/DLModel/run_mlm.py \\\n",
        "    --model_name_or_path $pretrain_model \\\n",
        "    --model_type bert \\\n",
        "    --train_file $train_data \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --eval_steps 10000 \\\n",
        "    --validation_file $valid_data \\\n",
        "    --max_seq_length 64 \\\n",
        "    --num_train_epochs 5.0 \\\n",
        "    --per_device_train_batch_size 128 \\\n",
        "    --gradient_accumulation_steps 16 \\\n",
        "    --save_steps 5000 \\\n",
        "    --preprocessing_num_workers 10 \\\n",
        "    --learning_rate 5e-5 \\\n",
        "    --output_dir /content/output_qalog \\\n",
        "    --overwrite_output_dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jAfV-RXqM8xH"
      },
      "outputs": [],
      "source": [
        "# json转换\n",
        "import json\n",
        "with open(\"qa_log_train.json\",\"w\") as fout:\n",
        "    with open(\"/content/NLP-Series-NewWordsMining-PTMPretraining/data/test.txt\",\"r\") as f1:\n",
        "        with open(\"/content/NLP-Series-NewWordsMining-PTMPretraining/ref/ref.txt\",\"r\") as f2:\n",
        "            for x,y in zip(f1.readlines(),f2.readlines()):\n",
        "                y = json.loads(y.strip(\"\\n\"))\n",
        "                out_dict = {\"text\":x.strip(\"\\n\"),\"chinese_ref\":y}\n",
        "                out_dict = json.dumps(out_dict)\n",
        "                fout.write(out_dict+\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YhLOu9zd9Gkk"
      },
      "source": [
        "# 情感分析"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "_TVYdfKFyWuU"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import csv\n",
        "import torch.nn as nn\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from transformers import BertTokenizer,BertConfig,AdamW\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "from tqdm import tqdm\n",
        "import transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vsLIgkaay8eW",
        "outputId": "c0a1ff11-853e-4126-d06d-df618e9a5322"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['1182.16400,1,25,16777215,1625551930,0,ccb92cf,51595054130659335'\n",
            "  '日元最高一万面值，所以只有两万张' 1]\n",
            " ['870.09500,1,25,16777215,1625551800,0,31976253,51594985658646535'\n",
            "  '前摇真长' 0]\n",
            " ['46.85700,1,25,16777215,1625551753,0,b7ac3164,51594960833609733'\n",
            "  '这剧看了两遍 很好看' 0]\n",
            " ...\n",
            " ['250.23400,1,25,16777215,1625487973,0,b4719c71,51561522039619591'\n",
            "  '房子好漂亮' 0]\n",
            " ['17.69900,1,25,16777215,1625487973,0,8f24000d,51561521901207557'\n",
            "  '绝命毒师呢？' 1]\n",
            " ['718.97900,1,25,16777215,1625487973,0,62b2ddef,51561522164924423'\n",
            "  '每做一件事情 自然有目的' 0]]\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "#自定义数据集类，torch.utils.data.random_split() 划分训练集、验证集、测试集。\n",
        " \n",
        "class MyDataSet(Dataset):\n",
        "    def __init__(self, loaded_data):\n",
        "        self.data = loaded_data\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        " \n",
        "Data_path = \"/content/novelWordDiscovery_BERT-wwm_emotionAnalysis/NovelWordDiscovery/Data/BV19X4y1c7uM.csv\"\n",
        "Totle_data = pd.read_csv(Data_path)\n",
        "Totle_data = Totle_data[Totle_data['B'].isin([0,1])]  \n",
        " \n",
        "#custom_dataset = MyDataSet(Totle_data)\n",
        "custom_dataset=Totle_data.values\n",
        "print(custom_dataset)\n",
        "#按照比例划分\n",
        "train_size = int(len(custom_dataset) * 0.8)\n",
        "validate_size = int(len(custom_dataset) * 0.1)\n",
        "test_size = len(custom_dataset) - validate_size - train_size\n",
        "train_dataset, validate_dataset, test_dataset = torch.utils.data.random_split(custom_dataset, [train_size, validate_size, test_size])\n",
        " \n",
        "#设置保存路径\n",
        "train_data_path=\"/content/NLP-Series-NewWordsMining-PTMPretraining/Data/JD_Bert_Try.csv\"\n",
        "dev_data_path = \"/content/NLP-Series-NewWordsMining-PTMPretraining/Data/JD_Bert_Dev.csv\" \n",
        "test_data_path=\"/content/NLP-Series-NewWordsMining-PTMPretraining/Data/JD_Bert_Test.csv\"\n",
        "#index参数设置为False表示不保存行索引,header设置为False表示不保存列索引\n",
        "test = pd.DataFrame(columns = ['A','Content','B'],data=list(train_dataset))\n",
        "test.to_csv(train_data_path,index=False,header=True)\n",
        "test = pd.DataFrame(columns = ['A','Content','B'],data=list(validate_dataset))\n",
        "test.to_csv(dev_data_path,index=False,header=True)\n",
        "test = pd.DataFrame(columns = ['A','Content','B'],data=list(test_dataset))\n",
        "test.to_csv(test_data_path,index=False,header=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "52YhYF550SiM"
      },
      "outputs": [],
      "source": [
        "class BertClassificationModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BertClassificationModel, self).__init__()   \n",
        "        #加载预训练模型\n",
        "        pretrained_weights=\"/content/output_qalog\"\n",
        "        self.bert = transformers.BertModel.from_pretrained(pretrained_weights)\n",
        "        for param in self.bert.parameters():\n",
        "            param.requires_grad = True\n",
        "        #定义线性函数      \n",
        "        self.dense = nn.Linear(768, 2)  #bert默认的隐藏单元数是768， 输出单元是2，表示二分类\n",
        "        \n",
        "    def forward(self, input_ids,token_type_ids,attention_mask):\n",
        "        #得到bert_output\n",
        "        bert_output = self.bert(input_ids=input_ids,token_type_ids=token_type_ids, attention_mask=attention_mask)\n",
        "        #获得预训练模型的输出\n",
        "        bert_cls_hidden_state = bert_output[1]\n",
        "        #将768维的向量输入到线性层映射为二维向量\n",
        "        linear_output = self.dense(bert_cls_hidden_state)\n",
        "        return  linear_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "3PCeQviB0VBu"
      },
      "outputs": [],
      "source": [
        "def encoder(max_len,vocab_path,text_list):\n",
        "    #将text_list embedding成bert模型可用的输入形式\n",
        "    #加载分词模型\n",
        "    tokenizer = BertTokenizer.from_pretrained(vocab_path)\n",
        "    tokenizer = tokenizer(\n",
        "        text_list,\n",
        "        padding = True,\n",
        "        truncation = True,\n",
        "        max_length = max_len,\n",
        "        return_tensors='pt'  # 返回的类型为pytorch tensor\n",
        "        )\n",
        "    input_ids = tokenizer['input_ids']\n",
        "    token_type_ids = tokenizer['token_type_ids']\n",
        "    attention_mask = tokenizer['attention_mask']\n",
        "    return input_ids,token_type_ids,attention_mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "nxgUItXa0eon"
      },
      "outputs": [],
      "source": [
        "def load_data(path):\n",
        "    csvFileObj = open(path)\n",
        "    readerObj = csv.reader(csvFileObj)\n",
        "    text_list = []\n",
        "    labels = []\n",
        "    for row in readerObj:\n",
        "        #跳过表头\n",
        "        if readerObj.line_num == 1:\n",
        "            continue\n",
        "        #label在什么位置就改成对应的index\n",
        "        label = int(row[2])\n",
        "        text = row[1]\n",
        "        text_list.append(text)\n",
        "        labels.append(label)\n",
        "    #调用encoder函数，获得预训练模型的三种输入形式\n",
        "    input_ids,token_type_ids,attention_mask = encoder(max_len=150,vocab_path=\"/content/output_qalog/vocab.txt\",text_list=text_list)\n",
        "    labels = torch.tensor(labels)\n",
        "    #将encoder的返回值以及label封装为Tensor的形式\n",
        "    data = TensorDataset(input_ids,token_type_ids,attention_mask,labels)\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R1X35ryj1CzC",
        "outputId": "cd6c8f99-e3c4-488b-c67f-6c54072ef6f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:1660: FutureWarning: Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
            "  FutureWarning,\n"
          ]
        }
      ],
      "source": [
        "#设定batch_size\n",
        "batch_size = 16\n",
        "#引入数据路径\n",
        "train_data_path=\"/content/NLP-Series-NewWordsMining-PTMPretraining/Data/JD_Bert_Try.csv\"\n",
        "dev_data_path = \"/content/NLP-Series-NewWordsMining-PTMPretraining/Data/JD_Bert_Dev.csv\" \n",
        "test_data_path=\"/content/NLP-Series-NewWordsMining-PTMPretraining/Data/JD_Bert_Test.csv\"\n",
        "#调用load_data函数，将数据加载为Tensor形式\n",
        "train_data = load_data(train_data_path)\n",
        "dev_data = load_data(dev_data_path)\n",
        "test_data = load_data(test_data_path)\n",
        "#将训练数据和测试数据进行DataLoader实例化\n",
        "train_loader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)\n",
        "dev_loader = DataLoader(dataset=dev_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_data, batch_size=batch_size, shuffle=False) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "HNfcWQtU1DTB"
      },
      "outputs": [],
      "source": [
        "def dev(model,dev_loader):\n",
        "    #将模型放到服务器上\n",
        "    model.to(device)\n",
        "#设定模式为验证模式\n",
        "    model.eval()\n",
        "#设定不会有梯度的改变仅作验证\n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for step, (input_ids,token_type_ids,attention_mask,labels) in tqdm(enumerate(dev_loader),desc='Dev Itreation:'):             \n",
        "          input_ids,token_type_ids,attention_mask,labels=input_ids.to(device),token_type_ids.to(device),attention_mask.to(device),labels.to(device)\n",
        "          out_put = model(input_ids,token_type_ids,attention_mask)\n",
        "          _, predict = torch.max(out_put.data, 1)\n",
        "          correct += (predict==labels).sum().item()\n",
        "          total += labels.size(0)\n",
        "        res = correct / total\n",
        "        return res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "aWfABTam1IJi"
      },
      "outputs": [],
      "source": [
        "def train(model,train_loader,dev_loader) :\n",
        "    #将model放到服务器上\n",
        "    model.to(device)\n",
        "    #设定模型的模式为训练模式\n",
        "    model.train()\n",
        "    #定义模型的损失函数\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    param_optimizer = list(model.named_parameters())\n",
        "    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
        "    #设置模型参数的权重衰减\n",
        "    optimizer_grouped_parameters = [\n",
        "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "         'weight_decay': 0.01},\n",
        "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "    ]\n",
        "    #学习率的设置\n",
        "    optimizer_params = {'lr': 1e-5, 'eps': 1e-6, 'correct_bias': False}\n",
        "    #使用AdamW 主流优化器\n",
        "    optimizer = AdamW(optimizer_grouped_parameters, **optimizer_params)\n",
        "    #学习率调整器，检测准确率的状态，然后衰减学习率\n",
        "    scheduler = ReduceLROnPlateau(optimizer,mode='max',factor=0.5,min_lr=1e-7, patience=5,verbose= True, threshold=0.0001, eps=1e-08)\n",
        "    t_total = len(train_loader)\n",
        "    #设定训练轮次\n",
        "    total_epochs = 2\n",
        "    bestAcc = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    print('Training and verification begin!')\n",
        "    for epoch in range(total_epochs): \n",
        "        for step, (input_ids,token_type_ids,attention_mask,labels) in enumerate(train_loader):\n",
        "#从实例化的DataLoader中取出数据，并通过 .to(device)将数据部署到服务器上    input_ids,token_type_ids,attention_mask,labels=input_ids.to(device),token_type_ids.to(device),attention_mask.to(device),labels.to(device)\n",
        "            #梯度清零\n",
        "            optimizer.zero_grad()\n",
        "            #将数据输入到模型中获得输出\n",
        "            out_put =  model(input_ids,token_type_ids,attention_mask)\n",
        "            #计算损失\n",
        "            loss = criterion(out_put, labels)\n",
        "            _, predict = torch.max(out_put.data, 1)\n",
        "            correct += (predict == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "             #每两步进行一次打印\n",
        "            if (step + 1) % 2 == 0:\n",
        "                train_acc = correct / total\n",
        "                print(\"Train Epoch[{}/{}],step[{}/{}],tra_acc{:.6f} %,loss:{:.6f}\".format(epoch + 1, total_epochs, step + 1, len(train_loader),train_acc*100,loss.item()))\n",
        "            #每五十次进行一次验证\n",
        "            if (step + 1) % 50 == 0:\n",
        "                train_acc = correct / total\n",
        "                #调用验证函数dev对模型进行验证，并将有效果提升的模型进行保存\n",
        "                acc = dev(model, dev_loader)\n",
        "                if bestAcc < acc:\n",
        "                    bestAcc = acc\n",
        "                    #模型保存路径\n",
        "                    path = '/root/data/savedmodel/span_bert_hide_model1.pkl'\n",
        "                    torch.save(model, path)\n",
        "                print(\"DEV Epoch[{}/{}],step[{}/{}],tra_acc{:.6f} %,bestAcc{:.6f}%,dev_acc{:.6f} %,loss:{:.6f}\".format(epoch + 1, total_epochs, step + 1, len(train_loader),train_acc*100,bestAcc*100,acc*100,loss.item()))\n",
        "        scheduler.step(bestAcc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uXmb2e8k3Fg6",
        "outputId": "ae8707d8-a1f7-4e7b-e0ad-7862f80837f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1\n"
          ]
        }
      ],
      "source": [
        "n_gpu = torch.cuda.device_count()\n",
        "print(n_gpu)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 957
        },
        "id": "NvW_jINr1LGh",
        "outputId": "a70238c4-649b-4bfc-b683-d4b8ad4b3dba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at /content/output_qalog were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertModel were not initialized from the model checkpoint at /content/output_qalog and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training and verification begin!\n",
            "Train Epoch[1/2],step[2/89],tra_acc53.125000 %,loss:0.721333\n",
            "Train Epoch[1/2],step[4/89],tra_acc57.812500 %,loss:0.620953\n",
            "Train Epoch[1/2],step[6/89],tra_acc66.666667 %,loss:0.476155\n",
            "Train Epoch[1/2],step[8/89],tra_acc67.968750 %,loss:0.560992\n",
            "Train Epoch[1/2],step[10/89],tra_acc70.000000 %,loss:0.364149\n",
            "Train Epoch[1/2],step[12/89],tra_acc72.916667 %,loss:0.298979\n",
            "Train Epoch[1/2],step[14/89],tra_acc75.000000 %,loss:0.462232\n",
            "Train Epoch[1/2],step[16/89],tra_acc75.781250 %,loss:0.326136\n",
            "Train Epoch[1/2],step[18/89],tra_acc75.000000 %,loss:0.488786\n",
            "Train Epoch[1/2],step[20/89],tra_acc74.687500 %,loss:0.556938\n",
            "Train Epoch[1/2],step[22/89],tra_acc75.852273 %,loss:0.380271\n",
            "Train Epoch[1/2],step[24/89],tra_acc76.041667 %,loss:0.373694\n",
            "Train Epoch[1/2],step[26/89],tra_acc76.682692 %,loss:0.266584\n",
            "Train Epoch[1/2],step[28/89],tra_acc77.232143 %,loss:0.294860\n",
            "Train Epoch[1/2],step[30/89],tra_acc77.500000 %,loss:0.302089\n",
            "Train Epoch[1/2],step[32/89],tra_acc78.320312 %,loss:0.376134\n",
            "Train Epoch[1/2],step[34/89],tra_acc78.308824 %,loss:0.558880\n",
            "Train Epoch[1/2],step[36/89],tra_acc78.819444 %,loss:0.287260\n",
            "Train Epoch[1/2],step[38/89],tra_acc79.605263 %,loss:0.187521\n",
            "Train Epoch[1/2],step[40/89],tra_acc80.312500 %,loss:0.264308\n",
            "Train Epoch[1/2],step[42/89],tra_acc80.952381 %,loss:0.262168\n",
            "Train Epoch[1/2],step[44/89],tra_acc81.392045 %,loss:0.307045\n",
            "Train Epoch[1/2],step[46/89],tra_acc81.793478 %,loss:0.209869\n",
            "Train Epoch[1/2],step[48/89],tra_acc82.031250 %,loss:0.431161\n",
            "Train Epoch[1/2],step[50/89],tra_acc82.250000 %,loss:0.170701\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Dev Itreation:: 12it [00:14,  1.21s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-f731f078162a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertClassificationModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#调用训练函数进行训练与验证\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdev_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-39-f857ee1ca23c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, dev_loader)\u001b[0m\n\u001b[1;32m     54\u001b[0m                     \u001b[0;31m#模型保存路径\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m                     \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/root/data/savedmodel/span_bert_hide_model1.pkl'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m                     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"DEV Epoch[{}/{}],step[{}/{}],tra_acc{:.6f} %,bestAcc{:.6f}%,dev_acc{:.6f} %,loss:{:.6f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_acc\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbestAcc\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbestAcc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    375\u001b[0m     \u001b[0m_check_dill_version\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpickle_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_use_new_zipfile_serialization\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/root/data/savedmodel/span_bert_hide_model1.pkl'"
          ]
        }
      ],
      "source": [
        "#device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "device = torch.device('cpu')\n",
        "#实例化模型\n",
        "model = BertClassificationModel()\n",
        "#调用训练函数进行训练与验证\n",
        "train(model,train_loader,dev_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kLFRyomo1OJQ"
      },
      "outputs": [],
      "source": [
        "def predict(model,test_loader):\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    predicts = []\n",
        "    predict_probs = []\n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for step, (input_ids,token_type_ids,attention_mask,labels) in enumerate(test_loader): \n",
        "            input_ids,token_type_ids,attention_mask,labels=input_ids.to(device),token_type_ids.to(device),attention_mask.to(device),labels.to(device)\n",
        "            out_put = model(input_ids,token_type_ids,attention_mask)\n",
        "           \n",
        "            _, predict = torch.max(out_put.data, 1)\n",
        " \n",
        "            pre_numpy = predict.cpu().numpy().tolist()\n",
        "            predicts.extend(pre_numpy)\n",
        "            probs = F.softmax(out_put).detach().cpu().numpy().tolist()\n",
        "            predict_probs.extend(probs)\n",
        " \n",
        "            correct += (predict==labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "        res = correct / total\n",
        "        print('predict_Accuracy : {} %'.format(100 * res))\n",
        "        #返回预测结果和预测的概率\n",
        "        return predicts,predict_probs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tZSEOeKv1RT-"
      },
      "outputs": [],
      "source": [
        "#引进训练好的模型进行测试\n",
        "path = '/root/data/savedmodel/span_bert_hide_model.pkl'\n",
        "Trained_model = torch.load(path)\n",
        "#predicts是预测的（0   1），predict_probs是概率值\n",
        "predicts,predict_probs = predict(Trained_model,dev_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Am2TfBS1Rrw"
      },
      "outputs": [],
      "source": [
        "P = sklearn.metrics.precision_score(y_true, y_pred, average=’binary’,sample_weight=None)\n",
        "R = sklearn.metrics.recall_score(y_true, y_pred, average=’binary’,sample_weight=None)\n",
        "F1 = sklearn.metrics.f1_score(y_true, y_pred,average=’binary’,sample_weight=None)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "newWord+bert-wmm+emotionAnalysis.ipynb",
      "provenance": [],
      "mount_file_id": "17eRbrWTqonFurzzHMQ5cWadqkhCBuHk4",
      "authorship_tag": "ABX9TyPBbn2gDwSaMjp7kJBSuLtd",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}